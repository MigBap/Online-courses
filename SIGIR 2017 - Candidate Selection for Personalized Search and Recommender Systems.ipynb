{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIGIR 2017 - Candidate Selection for Personalized Search and Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/candidate-selection-tutorial-sigir2017/candidate-selection-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**\n",
    "\n",
    "Modern day social media search and recommender systems require complex query formulation that incorporates both user context and their explicit search queries. Users expect these systems to be fast and provide relevant results to their query and context. With millions of documents to choose from, these systems utilize a **multi-pass scoring function to narrow the results** and provide the most relevant ones to users. **Candidate selection** is required to sift through all the documents in the index and select a relevant few to be ranked by subsequent scoring functions. It becomes crucial to narrow down the document set while maintaining relevant ones in resulting set. In this tutorial we survey various candidate selection techniques and deep dive into case studies on a large scale social media platform. In the later half we provide hands-on tutorial where we explore building these candidate selection models on a real world dataset and see how to balance the tradeoff between relevance and latency.\n",
    "\n",
    "**Presenters**\n",
    "Dhruv Arya, Ganesh Venkataraman, Aman Grover, Krishnaram Kenthapadi, Yiqun Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 'GPE'),\n",
       " ('Christine Lagarde', 'PERSON'),\n",
       " ('the Wall \\nStreet Journal', 'ORG')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meu\n",
    "from nltk.tag import StanfordNERTagger  # eles usaram isto para a NER\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall \n",
    "Street Journal.\"\"\"\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "\n",
    "#################  tentar depois melhor com o StanfordNERTagger ##########################\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "[(ent.text, ent.label_) for ent in nlp(text).ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our tutorial we will be using the following dependencies:\n",
    "import pandas as pd\n",
    "import pysolr\n",
    "import web\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "from nltk.tag import StanfordNERTagger  # usaram isto para a NER mais tarde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 0 & 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assignment 0: https://github.com/candidate-selection-tutorial-sigir2017/candidate-selection-tutorial/tree/master/assignments/assignment0\n",
    "- Assignment 1: https://github.com/candidate-selection-tutorial-sigir2017/candidate-selection-tutorial/tree/master/assignments/assignment1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "We will be using an open source News Aggregator Dataset. It references to news pages collected from a web aggregator in the period from 10-March-2014 to 10-August-2014. The resources are grouped into clusters that represent pages discussing the same story.\n",
    "\n",
    "Full details about the dataset can be found at UCI Machine Learning Repository - News Aggregator Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://archive.ics.uci.edu/ml/datasets/News+Aggregator#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dxyGGb4iN9Cs9aMZTKQpJeoiQfruM</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>b</td>\n",
       "      <td>http://techcrunch.com/ http://techcrunch.com/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dxyGGb4iN9Cs9aMZTKQpJeoiQfruM</td>\n",
       "      <td>techcrunch.com</td>\n",
       "      <td>b</td>\n",
       "      <td>http://techcrunch.com/ecommerce/ http://techcr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dxyGGb4iN9Cs9aMZTKQpJeoiQfruM</td>\n",
       "      <td>www.bnn.ca</td>\n",
       "      <td>b</td>\n",
       "      <td>http://www.bnn.ca/News/2014/ http://www.bnn.ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dxyGGb4iN9Cs9aMZTKQpJeoiQfruM</td>\n",
       "      <td>www.bnn.ca</td>\n",
       "      <td>b</td>\n",
       "      <td>http://www.bnn.ca/news http://www.bnn.ca/News/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dxyGGb4iN9Cs9aMZTKQpJeoiQfruM</td>\n",
       "      <td>www.bnn.ca</td>\n",
       "      <td>b</td>\n",
       "      <td>http://www.bnn.ca/News/News-Listing.aspx?Secto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0               1  2  \\\n",
       "0  dxyGGb4iN9Cs9aMZTKQpJeoiQfruM  techcrunch.com  b   \n",
       "1  dxyGGb4iN9Cs9aMZTKQpJeoiQfruM  techcrunch.com  b   \n",
       "2  dxyGGb4iN9Cs9aMZTKQpJeoiQfruM      www.bnn.ca  b   \n",
       "3  dxyGGb4iN9Cs9aMZTKQpJeoiQfruM      www.bnn.ca  b   \n",
       "4  dxyGGb4iN9Cs9aMZTKQpJeoiQfruM      www.bnn.ca  b   \n",
       "\n",
       "                                                   3  \n",
       "0  http://techcrunch.com/ http://techcrunch.com/2...  \n",
       "1  http://techcrunch.com/ecommerce/ http://techcr...  \n",
       "2  http://www.bnn.ca/News/2014/ http://www.bnn.ca...  \n",
       "3  http://www.bnn.ca/news http://www.bnn.ca/News/...  \n",
       "4  http://www.bnn.ca/News/News-Listing.aspx?Secto...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('2pageSessions.csv', sep='\\t', header=None, engine='python')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 45072: Expected 1 fields in line 45072, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 318471: Expected 1 fields in line 318471, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1\\tFed official says weak data caused by weath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2\\tFed's Charles Plosser sees high bar for cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3\\tUS open: Stocks fall after Fed official hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4\\tFed risks falling 'behind the curve', Charl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5\\tFed's Plosser: Nasty Weather Has Curbed Job...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  1\\tFed official says weak data caused by weath...\n",
       "1  2\\tFed's Charles Plosser sees high bar for cha...\n",
       "2  3\\tUS open: Stocks fall after Fed official hin...\n",
       "3  4\\tFed risks falling 'behind the curve', Charl...\n",
       "4  5\\tFed's Plosser: Nasty Weather Has Curbed Job..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_csv('newsCorpora.csv', sep='delimiter', header=None, engine='python', error_bad_lines=False)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421493, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STORY</th>\n",
       "      <th>HOSTNAME</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.latimes.com</td>\n",
       "      <td>1394470370698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.livemint.com</td>\n",
       "      <td>1394470371207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/us-open-stocks...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1  Fed official says weak data caused by weather,...   \n",
       "1   2  Fed's Charles Plosser sees high bar for change...   \n",
       "2   3  US open: Stocks fall after Fed official hints ...   \n",
       "\n",
       "                                                 URL          PUBLISHER  \\\n",
       "0  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
       "1  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
       "2  http://www.ifamagazine.com/news/us-open-stocks...       IFA Magazine   \n",
       "\n",
       "  CATEGORY                          STORY             HOSTNAME      TIMESTAMP  \n",
       "0        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM      www.latimes.com  1394470370698  \n",
       "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM     www.livemint.com  1394470371207  \n",
       "2        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371550  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HEADERS = [\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "\n",
    "# check data\n",
    "df_news = pd.read_csv('newsCorpora.csv', sep='\\t', header=None, engine='python', \n",
    "                      error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "# shape\n",
    "print(df_news.shape)\n",
    "\n",
    "# display head\n",
    "df_news.columns = HEADERS.copy()\n",
    "display(df_news.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup solr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lucene.apache.org/solr/guide/6_6/getting-started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup a solr instance and create index schema for the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "from subprocess import call\n",
    "import argparse\n",
    "import csv\n",
    "import pysolr\n",
    "\n",
    "INDEX_NAME = 'simpleindex'\n",
    "INDEX_MAP = [\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "SOLR_URL = 'http://localhost:8983/solr'\n",
    "\n",
    "data_folder = \"\"\"C:/Users/Admin/OneDrive/Ikari_Technology_Solutions/Tutorial_SIGIR 2017 - Candidate Selection for Personalized Search and Recommender Systems/\"\"\"\n",
    "\n",
    "# pasta onde está instalado o solr -> ir até ao comando na pasta bin\n",
    "solr_cmd = data_folder + \"solr-8.6.0/bin/solr.cmd\"\n",
    "\n",
    "# data file name\n",
    "df_news_csv = data_folder + 'newsCorpora.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para criar a estrutura de um documento a inserir no index\n",
    "def create_document(record):\n",
    "    \"\"\"\n",
    "    This function creates a representation for the document to be put in the solr index.\n",
    "    \"\"\"    \n",
    "    #Write an iterator over the INDEX_MAP to fetch fields from the record and return a dictionary representing the document.\n",
    "    document = {}\n",
    "    for idx, field in enumerate(INDEX_MAP):\n",
    "        if field.lower() == 'id':\n",
    "            document[field.lower()] = record[idx]\n",
    "        else:\n",
    "            document[\"_news_%s\" % (field.lower())] = record[idx].lower()\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pysolr.Solr object at 0x000001D093CFCDC8> \n",
      "\n",
      "['1', 'Fed official says weak data caused by weather, should not slow taper', 'http://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310,0,1312750.story\\\\?track=rss', 'Los Angeles Times', 'b', 'ddUyU0VZz0BRneMioxUPQVP6sIxvM', 'www.latimes.com', '1394470370698']\n"
     ]
    }
   ],
   "source": [
    "# solr object\n",
    "print(pysolr.Solr(url=\"%s/%s\" % (SOLR_URL, INDEX_NAME)), \"\\n\")\n",
    "\n",
    "# ver só o primeiro\n",
    "for i in csv.reader(open(df_news_csv, encoding='utf-8'), delimiter='\\t'):\n",
    "    if i[0] == '2':\n",
    "        break\n",
    "    else:\n",
    "        print(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1',\n",
       " '_news_title': 'fed official says weak data caused by weather, should not slow taper',\n",
       " '_news_url': 'http://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310,0,1312750.story\\\\?track=rss',\n",
       " '_news_publisher': 'los angeles times',\n",
       " '_news_category': 'b',\n",
       " '_news_story': 'dduyu0vzz0brnemioxupqvp6sixvm',\n",
       " '_news_hostname': 'www.latimes.com',\n",
       " '_news_timestamp': '1394470370698'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estrutura dos documentos (exemplo: 1.º)\n",
    "create_document(['1', 'Fed official says weak data caused by weather, should not slow taper', 'http://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310,0,1312750.story\\\\?track=rss', 'Los Angeles Times', 'b', 'ddUyU0VZz0BRneMioxUPQVP6sIxvM', 'www.latimes.com', '1394470370698'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para criar um core e inserir os documentos no index\n",
    "def index(input_file, num_records):\n",
    "    \"\"\"\n",
    "    Creates a representation of the document and puts the document in the solr index. \n",
    "    The index name is defined as a part of the url.\n",
    "    \"\"\"\n",
    "\n",
    "    # create the solr core (manualmente: na linha de comandos, em bin, \"solr create -c simple_index\")\n",
    "    call([\"{}\".format(solr_cmd), \"create\", \"-c\", INDEX_NAME])\n",
    "    \n",
    "    # Create a client instance\n",
    "    solr_interface = pysolr.Solr(url=\"%s/%s\" % (SOLR_URL, INDEX_NAME))\n",
    "    \n",
    "    # index data\n",
    "    with open(input_file, encoding='utf-8') as csvfile:\n",
    "        records = csv.reader(csvfile, delimiter='\\t')\n",
    "        batched_documents = []\n",
    "        for idx, record in enumerate(records):\n",
    "            if idx == num_records:\n",
    "                break\n",
    "            # Write code for creating the document and passing it for indexing\n",
    "            else:\n",
    "                batched_documents.append(create_document(record))\n",
    "                print(\"Added document %d to the %s index\" % (idx, INDEX_NAME))\n",
    "    \n",
    "    # and finally index the complete list (batched_documents)\n",
    "    solr_interface.add(batched_documents)\n",
    "            \n",
    "    # Commit the changes to the index after adding the documents\n",
    "    solr_interface.commit()\n",
    "    print('Finished adding the documents to the solr index')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.73 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# start the server (Ou, na linha de comandos, na pasta onde está instalado o solr: bin\\solr.cmd start )\n",
    "call([\"{}\".format(solr_cmd), \"start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added document 0 to the simpleindex index\n",
      "Added document 1 to the simpleindex index\n",
      "Added document 2 to the simpleindex index\n",
      "Added document 3 to the simpleindex index\n",
      "Added document 4 to the simpleindex index\n",
      "Added document 5 to the simpleindex index\n",
      "Added document 6 to the simpleindex index\n",
      "Added document 7 to the simpleindex index\n",
      "Added document 8 to the simpleindex index\n",
      "Added document 9 to the simpleindex index\n",
      "Added document 10 to the simpleindex index\n",
      "Added document 11 to the simpleindex index\n",
      "Added document 12 to the simpleindex index\n",
      "Added document 13 to the simpleindex index\n",
      "Added document 14 to the simpleindex index\n",
      "Finished adding the documents to the solr index\n",
      "Wall time: 6.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create the solr core and simple index\n",
    "index(df_news_csv, num_records=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8983/solr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go to the chrome app\n",
    "print(SOLR_URL)\n",
    "\n",
    "# solr interface created\n",
    "solr_interface = pysolr.Solr(url=SOLR_URL + \"/{}\".format(INDEX_NAME))\n",
    "\n",
    "# results - Search all (*) - by default there are presented 10 results\n",
    "len(solr_interface.search(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que fizemos acima (We will start by building a very basic document with prefix _news_ and utilizing pysolr for batch indexing the documents.) foi equivalente a ( ver tutorial em https://lucene.apache.org/solr/guide/8_6/solr-tutorial.html)\n",
    "\n",
    "\n",
    "curl -X POST -H 'Content-type:application/json' --data-binary '{\n",
    "  \"add-field-type\" : {\n",
    "     \"name\":\"simple_indexed_text\",\n",
    "     \"class\":\"solr.TextField\",\n",
    "     \"positionIncrementGap\":\"100\",\n",
    "     \"analyzer\" : {\n",
    "        \"tokenizer\":{ \n",
    "           \"class\":\"solr.WhitespaceTokenizerFactory\" }\n",
    "      }}\n",
    "}' http://localhost:8983/solr/simpleindex/schema\n",
    "\n",
    "curl -X POST -H 'Content-type:application/json' --data-binary '{\n",
    "  \"add-dynamic-field\":{\n",
    "     \"name\":\"_news_*\",\n",
    "     \"type\":\"simple_indexed_text\",\n",
    "     \"indexed\":true,\n",
    "     \"stored\":true }\n",
    "}' http://localhost:8983/solr/simpleindex/schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the News Aggregator Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of building a search index is to understand the dataset and the fields that you want to allow the user to search on. To do this we will read a first few records of the dataset into a tabular form. This will allow us to understand how does the data look like.\n",
    "\n",
    "This analysis allows us to understand to what degree are the following tasks needed\n",
    "\n",
    "- Tokenization and Segmentation\n",
    "- Term Normalization\n",
    "- Data transformation\n",
    "\n",
    "We have provided you with a basic script that prints out the data in a tabular form along with some statistics about field values.\n",
    "\n",
    "To run the script issue the following command or run it with arguments from PyCharm.\n",
    "\n",
    "cd ~/workspace/candidate-selection-tutorial/assignments/assignment1/exercise/src \n",
    "python understand_data.py --input /home/sigir/workspace/candidate-selection-tutorial/finished-product/data/news-aggregator-dataset/newsCorpora.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------+-------------------------------+------------------------------+---------------+\n",
      "| ID |                                  TITLE                                   |                                                                              URL                                                                               |      PUBLISHER       | CATEGORY |             STORY             |           HOSTNAME           |   TIMESTAMP   |\n",
      "+----+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------+-------------------------------+------------------------------+---------------+\n",
      "| 1  |   Fed official says weak data caused by weather, should not slow taper   |                  http://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310,0,1312750.story\\?track=rss                   |  Los Angeles Times   |    b     | ddUyU0VZz0BRneMioxUPQVP6sIxvM |       www.latimes.com        | 1394470370698 |\n",
      "| 2  |    Fed's Charles Plosser sees high bar for change in pace of tapering    |                    http://www.livemint.com/Politics/H2EvwJSK2VE6OF7iK1g3PP/Feds-Charles-Plosser-sees-high-bar-for-change-in-pace-of-ta.html                    |       Livemint       |    b     | ddUyU0VZz0BRneMioxUPQVP6sIxvM |       www.livemint.com       | 1394470371207 |\n",
      "| 3  |  US open: Stocks fall after Fed official hints at accelerated tapering   |                          http://www.ifamagazine.com/news/us-open-stocks-fall-after-fed-official-hints-at-accelerated-tapering-294436                           |     IFA Magazine     |    b     | ddUyU0VZz0BRneMioxUPQVP6sIxvM |     www.ifamagazine.com      | 1394470371550 |\n",
      "| 4  |        Fed risks falling 'behind the curve', Charles Plosser says        |                                 http://www.ifamagazine.com/news/fed-risks-falling-behind-the-curve-charles-plosser-says-294430                                 |     IFA Magazine     |    b     | ddUyU0VZz0BRneMioxUPQVP6sIxvM |     www.ifamagazine.com      | 1394470371793 |\n",
      "| 5  |            Fed's Plosser: Nasty Weather Has Curbed Job Growth            |                            http://www.moneynews.com/Economy/federal-reserve-charles-plosser-weather-job-growth/2014/03/10/id/557011                            |      Moneynews       |    b     | ddUyU0VZz0BRneMioxUPQVP6sIxvM |      www.moneynews.com       | 1394470372027 |\n",
      "| 6  |            Plosser: Fed May Have to Accelerate Tapering Pace             |                                 http://www.nasdaq.com/article/plosser-fed-may-have-to-accelerate-tapering-pace-20140310-00371                                  |        NASDAQ        |    b     | ddUyU0VZz0BRneMioxUPQVP6sIxvM |        www.nasdaq.com        | 1394470372212 |\n",
      "| 7  |                Fed's Plosser: Taper pace may be too slow                 |                           http://www.marketwatch.com/story/feds-plosser-taper-pace-may-be-too-slow-2014-03-10\\?reflink=MW_news_stmp                            |     MarketWatch      |    b     | ddUyU0VZz0BRneMioxUPQVP6sIxvM |     www.marketwatch.com      | 1394470372405 |\n",
      "| 8  | Fed's Plosser expects US unemployment to fall to 6.2% by the end of 2014 |                               http://www.fxstreet.com/news/forex-news/article.aspx\\?storyid=23285020-b1b5-47ed-a8c4-96124bb91a39                               |     FXstreet.com     |    b     | ddUyU0VZz0BRneMioxUPQVP6sIxvM |       www.fxstreet.com       | 1394470372615 |\n",
      "| 9  |  US jobs growth last month hit by weather:Fed President Charles Plosser  | http://economictimes.indiatimes.com/news/international/business/us-jobs-growth-last-month-hit-by-weatherfed-president-charles-plosser/articleshow/31788000.cms |    Economic Times    |    b     | ddUyU0VZz0BRneMioxUPQVP6sIxvM | economictimes.indiatimes.com | 1394470372792 |\n",
      "| 10 |       ECB unlikely to end sterilisation of SMP purchases - traders       |                                                     http://www.iii.co.uk/news-opinion/reuters/news/152615                                                      | Interactive Investor |    b     | dPhGU51DcrolUIMxbRm0InaHGA2XM |        www.iii.co.uk         | 1394470501265 |\n",
      "+----+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------+-------------------------------+------------------------------+---------------+\n",
      "\n",
      "Category values in the dataset:\n",
      "e    151736\n",
      "b    115926\n",
      "t    108241\n",
      "m     45590\n",
      "Name: CATEGORY, dtype: int64\n",
      "\n",
      "Most common hostnames in the dataset:\n",
      "in.reuters.com            2871\n",
      "www.huffingtonpost.com    2598\n",
      "www.businessweek.com      2420\n",
      "www.contactmusic.com      2304\n",
      "www.dailymail.co.uk       2258\n",
      "www.nasdaq.com            2228\n",
      "www.examiner.com          2083\n",
      "www.globalpost.com        1973\n",
      "www.latimes.com           1913\n",
      "www.bizjournals.com       1882\n",
      "Name: HOSTNAME, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# understand_data.py ---> faz isto:\n",
    "\n",
    "import csv\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "num_records = 10\n",
    "\n",
    "with open(df_news_csv) as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter='\\t')\n",
    "    pretty_table = PrettyTable()\n",
    "    pretty_table.field_names = HEADERS\n",
    "    for count, row in enumerate(csvreader):\n",
    "        if count == num_records:\n",
    "            break\n",
    "        pretty_table.add_row(row)\n",
    "    print(pretty_table)\n",
    "\n",
    "# Category values\n",
    "print('\\nCategory values in the dataset:')\n",
    "print(df_news.CATEGORY.value_counts())\n",
    "\n",
    "# hostnames (most common)\n",
    "print('\\nMost common hostnames in the dataset:')\n",
    "print(df_news.HOSTNAME.value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310',\n",
       " '0',\n",
       " '1312750.story\\\\?track=rss']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.iloc[0]['URL'].split(',')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4343\n"
     ]
    }
   ],
   "source": [
    "# os que têm vírgulas e mais informação para além do url:\n",
    "corrigir_url = []\n",
    "[corrigir_url.append(i) for i in range(len(df_news)) if len(df_news.iloc[i]['URL'].split(',')) > 1]\n",
    "\n",
    "# são mais..\n",
    "print(len(corrigir_url)) # ~1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stanford_title_ner_tags_case_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{\"ORGANIZATION\": [\"Fed\"]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>{\"PERSON\": [\"Charles Plosser\"]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>{\"ORGANIZATION\": [\"Fed\"], \"LOCATION\": [\"US\"]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>{\"PERSON\": [\"Charles Plosser\"]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                              1\n",
       "0  1                      {\"ORGANIZATION\": [\"Fed\"]}\n",
       "1  2                {\"PERSON\": [\"Charles Plosser\"]}\n",
       "2  3  {\"ORGANIZATION\": [\"Fed\"], \"LOCATION\": [\"US\"]}\n",
       "3  4                {\"PERSON\": [\"Charles Plosser\"]}\n",
       "4  5                                             {}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stanford_title_ner_tags_case_sensitive\n",
    "stanford_title_NER = pd.read_csv('stanford_title_ner_tags_case_sensitive.csv', sep='\\t', header=None, engine='python', \n",
    "                                 error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "stanford_title_NER.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422419\n",
      "421493\n"
     ]
    }
   ],
   "source": [
    "print(len(stanford_title_NER)); \n",
    "print(len(df_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fed official says weak data caused by weather, should not slow taper\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Fed', 'ORG')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ex = df_news['TITLE'][0]\n",
    "tokenized_text = word_tokenize(text_ex)\n",
    "\n",
    "#################  tentar depois melhor com o StanfordNERTagger ##########################\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(text_ex)\n",
    "[(ent.text, ent.label_) for ent in nlp(text_ex).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US open: Stocks fall after Fed official hints at accelerated tapering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('US', 'GPE'), ('Fed', 'ORG')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ex = df_news['TITLE'][2]\n",
    "tokenized_text = word_tokenize(text_ex)\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(text_ex)\n",
    "[(ent.text, ent.label_) for ent in nlp(text_ex).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fed's Plosser: Nasty Weather Has Curbed Job Growth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Fed', 'ORG')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ex = df_news['TITLE'][4]\n",
    "tokenized_text = word_tokenize(text_ex)\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(text_ex)\n",
    "[(ent.text, ent.label_) for ent in nlp(text_ex).ents]  # ----> este o stanfordNER não apanhou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Rewriting and Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will connect our middletier and the frontend to the index. We will accept the query from the search front end, rewrite the query to search our index and send the results back to the frontend for displaying.\n",
    "\n",
    "As next step open the file **frontend/app.py**. This is our middle tier that does serves the search requests and talks to the search backend. In this file you need to write the **GET** method of **SearchSimpleIndex** class.\n",
    "\n",
    "- **Match All Query** - This query will be useful for serving queries with no keywords. Refer to solr documentation on how to construct it.\n",
    "- **Text Based Query** - For queries with keywords we will make use of the catch all field in solr. All content to be indexed in a predefined \"catch-all\" \"_text_\" field, to enable single-field search that includes all fields' content. The query should look of the form:\n",
    "\n",
    "Query: la times <br>\n",
    "Tokens: ['la', 'times'] <br>\n",
    "Generated Query: _text_:la AND _text_:times\n",
    "\n",
    "**Running the server** <br>\n",
    "To see the search in action follow the commands below:\n",
    "\n",
    "cd frontend<br>\n",
    "python app.py <br>\n",
    "\n",
    "This should run the simple index search server on http://0.0.0.0:8080. The page should look like the image below. Try out some queries to see if you are getting results back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['la', 'times']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"la times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import web\n",
    "import pysolr\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# MUDEI O CÓDIGO - RETIREI A PARTE draw E NÃO USEI get_web_input\n",
    "\n",
    "urls = ('/', 'SimpleIndexSearchPage', '/searchSimpleIndex', 'SearchSimpleIndex',)\n",
    "\n",
    "CATEGORY = {'b': 'Business', 'e': 'Entertainment', 't': 'Science and Technology', 'm': 'Health'}\n",
    "# render = web.template.render(data_folder + \"candidate-selection-tutorial/assignments/assignment1/exercise/src/frontend/templates/\",\n",
    "#                              base='layout')\n",
    "SOLR_SIMPLEINDEX = pysolr.Solr('http://localhost:8983/solr/simpleindex')\n",
    "\n",
    "\n",
    "def get_web_input(web_input):\n",
    "    draw = web_input['draw']\n",
    "    query = web_input['search[value]']\n",
    "    offset = web_input['start']\n",
    "    count = web_input['length']\n",
    "    return draw, query, offset, count\n",
    "\n",
    "\n",
    "def search(query, offset, count, solr_endpoint):\n",
    "    \"\"\"\n",
    "    This function is responsible for hitting the solr endpoint and returning the results back.\n",
    "    \"\"\"\n",
    "    results = solr_endpoint.search(q=query, **{'start': int(offset), 'rows': int(count)})\n",
    "    print(\"Saw {0} result(s) for query {1}.\".format(len(results), query))\n",
    "    \n",
    "    formatted_hits = []\n",
    "    for hit in results.docs:\n",
    "        formatted_hits.append(\n",
    "            [hit['_news_title'], hit['_news_publisher'], CATEGORY[hit['_news_category'][0]], hit['_news_url']])\n",
    "    response = {'recordsFiltered': results.hits,\n",
    "                'data': formatted_hits}\n",
    "#     web.header('Content-Type', 'application/json')\n",
    "    return json.dumps(response)\n",
    "\n",
    "\n",
    "class SimpleIndexSearchPage:\n",
    "    def GET(self):\n",
    "        return render.simpleIndexSearchPage()\n",
    "\n",
    "\n",
    "class SearchSimpleIndex:\n",
    "    def GET(self):\n",
    "        query, offset, count = get_web_input(web_input=web.input())\n",
    "        # TODO: Write code for handling the empty query (no keywords)\n",
    "        if query == '*:*':\n",
    "            return search(query=query, offset=offset, count=count)\n",
    "        \n",
    "        # TODO: Write code for tokenizing the search query and creating must clauses for each token\n",
    "        clauses = []\n",
    "        for token in word_tokenize(query):\n",
    "            clauses.append(\"+_text_:%s\" % token)\n",
    "        query = \" AND \".join(clauses)\n",
    "        return search(query=query, offset=offset, count=count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw 1 result(s) for query _news_publisher:livemint.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"recordsFiltered\": 1, \"data\": [[[\"fed\\'s charles plosser sees high bar for change in pace of tapering\"], [\"livemint\"], \"Business\", [\"http://www.livemint.com/politics/h2evwjsk2ve6of7ik1g3pp/feds-charles-plosser-sees-high-bar-for-change-in-pace-of-ta.html\"]]]}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"_news_publisher:livemint\", offset=0, count=10, solr_endpoint=SOLR_SIMPLEINDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw 2 result(s) for query _news_publisher:times.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"recordsFiltered\": 2, \"data\": [[[\"us jobs growth last month hit by weather:fed president charles plosser\"], [\"economic times\"], \"Business\", [\"http://economictimes.indiatimes.com/news/international/business/us-jobs-growth-last-month-hit-by-weatherfed-president-charles-plosser/articleshow/31788000.cms\"]], [[\"fed official says weak data caused by weather, should not slow taper\"], [\"los angeles times\"], \"Business\", [\"http://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310,0,1312750.story\\\\\\\\?track=rss\"]]]}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"_news_publisher:times\", offset=0, count=10, solr_endpoint=SOLR_SIMPLEINDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw 1 result(s) for query _news_publisher:times AND _news_title:jobs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"recordsFiltered\": 1, \"data\": [[[\"us jobs growth last month hit by weather:fed president charles plosser\"], [\"economic times\"], \"Business\", [\"http://economictimes.indiatimes.com/news/international/business/us-jobs-growth-last-month-hit-by-weatherfed-president-charles-plosser/articleshow/31788000.cms\"]]]}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"_news_publisher:times AND _news_title:jobs\", offset=0, count=10, solr_endpoint=SOLR_SIMPLEINDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1e+03 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#poderia ajustar a função dps (o search é o mesmo que fazer .contains('x')?)\n",
    "len([i for i in df_news['TITLE'][:10] if 'Fed' in i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/candidate-selection-tutorial-sigir2017/candidate-selection-tutorial/tree/master/assignments/assignment2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will be building a better index with fields specific to the entities recognized in the title. We will make use of Stanford NER along NLTK. In addition to building the index we will work on utilizing entities in the incoming query and writing a field specific query matching entities in the query with the fields containing those entities in the search index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Search Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use our learnings from the previous assignment to build the entity based search index. To utilize the time better we have pregenerated the entity tags using the **Stanford NER** library and english.all.3class.distsim.crf.ser.gz classifier. The classifier provides three tags namely\n",
    "\n",
    "PERSON <br>\n",
    "ORGANIZATION <br>\n",
    "LOCATION <br>\n",
    "\n",
    "When building the index we will read through the tags and our dataset simultaneously. This will allow us to use the pregenerated tags when building the document to be indexed.\n",
    "\n",
    "Open the file **entity_aware_index.py** and you will need to implement the following parts\n",
    "\n",
    "- Writing the function for creating the document. Similar to Assignment 1 we will be building a dictionary with all the index fields. Our focus here will be to add additional title fields specifically for the NER tags. The specific fields **_news_title_person, _news_title_organization and _news_title_location** need to be added in addition to **_news_title**.\n",
    "\n",
    "Your documents should have a structure similar to the one below -\n",
    "\n",
    "{\n",
    "\t\"_news_url\": \"http://www.ifamagazine.com/news/us-open-stocks-fall-after-fed-official-hints-at-accelerated-tapering-294436\",<br>\n",
    "\t\"_news_title_organization\": \"Fed\",<br>\n",
    "\t\"_news_title\": \"us open: stocks fall after fed official hints at accelerated tapering\",<br>\n",
    "\t\"_news_story\": \"dduyu0vzz0brnemioxupqvp6sixvm\",<br>\n",
    "\t\"_news_category\": \"b\",<br>\n",
    "\t\"_news_hostname\": \"www.ifamagazine.com\",<br>\n",
    "\t\"_news_publisher\": \"ifa magazine\",<br>\n",
    "\t\"_news_timestamp\": \"1394470371550\",<br>\n",
    "\t\"id\": \"3\",<br>\n",
    "\t\"_news_title_location\": \"US\"<br>\n",
    "}<br>\n",
    "\n",
    "- The second task involves writing code for addition a document to the Solr index. You can reuse your code from Assignment 1 here.\n",
    "\n",
    "To begin indexing follow the commands listed below, you need to be in the assignment2 folder for running the commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "import argparse\n",
    "import json\n",
    "import csv\n",
    "import pysolr\n",
    "import gzip\n",
    "\n",
    "INDEX_MAP = [\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "SOLR_URL = 'http://localhost:8983/solr'\n",
    "\n",
    "# Location, Time, Person, Organization, Money, Percent, Date (Stanford NER)\n",
    "\n",
    "# Person, Norp (Nationalities or religious or political groups.), Facility, Org, GPE (Countries, cities, states.)\n",
    "# Loc (Non GPE Locations ex. mountain ranges, water), Product (Objects, vehicles, foods, etc. (Not services.),\n",
    "# EVENT (Named hurricanes, battles, wars, sports events, etc.), WORK_OF_ART (Titles of books, songs, etc), LANGUAGE\n",
    "# Refer to https://spacy.io/docs/usage/entity-recognition (SPACY NER)\n",
    "\n",
    "\n",
    "def create_document_ner(record, ner_tag):\n",
    "    \"\"\"\n",
    "    This function creates a representation for the document to be put in the solr index.\n",
    "    \"\"\"\n",
    "    document = {}\n",
    "    for idx, field in enumerate(INDEX_MAP):\n",
    "        if field.lower() == 'id':\n",
    "            document[field.lower()] = record[idx]\n",
    "        else:\n",
    "            document[\"_news_%s\" % (field.lower())] = record[idx].lower()\n",
    "            \n",
    "    # inserir _news_title_NERtag, ex. title_person, title_organization, title_location.\n",
    "    for i, j in enumerate(json.loads(ner_tag)):\n",
    "        document[\"_news_title_%s\" % (j.lower())] = list(json.loads(ner_tag).values())[i]\n",
    "\n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new index\n",
    "INDEX_NAME = 'entityawareindex'\n",
    "\n",
    "# function\n",
    "def index_ner(input_file, ner_tags_filename, num_records):\n",
    "    \"\"\"\n",
    "    Creates a representation of the document and puts the document in the solr index. \n",
    "    The index name is defined as a part of the url.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create the solr core \n",
    "    call([\"{}\".format(solr_cmd), \"create\", \"-c\", INDEX_NAME])\n",
    "    \n",
    "    # Create a client instance\n",
    "    solr_interface = pysolr.Solr(url=\"%s/%s\" % (SOLR_URL, INDEX_NAME))\n",
    "    \n",
    "    # index data\n",
    "    with open(input_file) as csvfile, open(ner_tags_filename) as ner_tags_file:\n",
    "        records = csv.reader(csvfile, delimiter='\\t')\n",
    "        ner_tags = csv.reader(ner_tags_file, delimiter='\\t')\n",
    "        batched_documents = []\n",
    "        for idx, (record, ner_tag) in enumerate(zip(records, ner_tags)):\n",
    "            if idx == num_records:\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                # não esquecer de pôr [1] em ner_tag (no ficheiro vem um algarismo sempre primeiro)\n",
    "                batched_documents.append(create_document_ner(record, ner_tag[1]))\n",
    "                print(\"Added document %d to the %s index\" % (idx, INDEX_NAME))\n",
    "    \n",
    "    # and finally index the complete list (batched_documents)\n",
    "    solr_interface.add(batched_documents)\n",
    "                \n",
    "    # Commit the changes to the index after adding the documents\n",
    "    solr_interface.commit()\n",
    "    print('Finished adding the documents to the solr index')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added document 0 to the entityawareindex index\n",
      "Added document 1 to the entityawareindex index\n",
      "Added document 2 to the entityawareindex index\n",
      "Added document 3 to the entityawareindex index\n",
      "Added document 4 to the entityawareindex index\n",
      "Added document 5 to the entityawareindex index\n",
      "Added document 6 to the entityawareindex index\n",
      "Added document 7 to the entityawareindex index\n",
      "Added document 8 to the entityawareindex index\n",
      "Added document 9 to the entityawareindex index\n",
      "Added document 10 to the entityawareindex index\n",
      "Added document 11 to the entityawareindex index\n",
      "Added document 12 to the entityawareindex index\n",
      "Added document 13 to the entityawareindex index\n",
      "Added document 14 to the entityawareindex index\n",
      "Finished adding the documents to the solr index\n",
      "Wall time: 4.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ner_tags_filename = 'stanford_title_ner_tags_case_sensitive.csv'\n",
    "\n",
    "# create the solr core and simple index\n",
    "index_ner(df_news_csv, ner_tags_filename, num_records=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8983/solr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go to the chrome app\n",
    "print(SOLR_URL)\n",
    "\n",
    "# solr interface created\n",
    "solr_interface = pysolr.Solr(url=SOLR_URL + \"/{}\".format(INDEX_NAME))\n",
    "\n",
    "# results - Search all (*) - by default there are presented 10 results\n",
    "len(solr_interface.search(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Rewriting and Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will define a query that is matched with the document on specific fields. We will make use of our entity understanding and utilize the Stanford NER server at runtime to generate tags.\n",
    "\n",
    "As next step open the file frontend/app.py. This is our middle tier that does serves the search requests and talks to the search backend. In this file you need to write the GET method of SearchEntityAwareIndex class.\n",
    "\n",
    "- **Match All Query** - Similar to Assignment 1 add the logic to serve results when the query is empty.\n",
    "- **Entity & Field Based Query** - For queries with keywords we will make use of the catch all field in solr. All content to be indexed in a predefined \"catch-all\" _text_ field, to enable single-field search that includes all fields' content. The query should look of the form:\n",
    "\n",
    "Query: cooperman paypal <br>\n",
    "Tokens: ['cooperman', 'paypal'] <br>\n",
    "NER Tags: {\"ORGANIZATION\": [\"PayPal\"], \"PERSON\": [\"Cooperman\"]} <br>\n",
    "Generated Query: _news_title_organization:paypal AND _news_title_person:Cooperman <br>\n",
    "\n",
    "**Helper Code Snippets** \n",
    "\n",
    "- Calling the Stanford NER server to get NER tags, accumulate_tags function in SearchEntityAwareIndex is provided for aggregating the NER tags.\n",
    "\n",
    "entity_tags = STANFORD_NER_SERVER.get_entities(query) <br>\n",
    "entity_tags = self.accumulate_tags(entity_tags)<br>\n",
    "\n",
    "- Boosting Paramter - You can pass in an optional boosting parameter of the form to boost matches in certain fields. Example\n",
    "\n",
    "qf = '_news_title_person^10 _news_title_organization^5 _news_title_location^100 _news_title^2.0 _news_publisher^10.0'\n",
    "\n",
    "**Running the server**\n",
    "To see the search in action follow the commands below:\n",
    "\n",
    "cd frontend <br>\n",
    "python app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import web\n",
    "import pysolr\n",
    "import string\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sner import Ner\n",
    "\n",
    "urls = (\n",
    "    '/', 'SimpleIndexSearchPage',\n",
    "    '/entityAwareSearchPage', 'EntityAwareSearch',\n",
    "    '/searchSimpleIndex', 'SearchSimpleIndex',\n",
    "    '/searchEntityAwareIndex', 'SearchEntityAwareIndex'\n",
    ")\n",
    "\n",
    "CATEGORY = {'b': 'Business', 'e': 'Entertainment', 't': 'Science and Technology', 'm': 'Health'}\n",
    "render = web.template.render('templates/', base='layout')\n",
    "SOLR_SIMPLEINDEX = pysolr.Solr('http://localhost:8983/solr/simpleindex')\n",
    "SOLR_ENTITYAWAREINDEX = pysolr.Solr('http://localhost:8983/solr/entityawareindex')\n",
    "STANFORD_NER_SERVER = Ner(host='localhost', port=9199)\n",
    "\n",
    "def get_web_input(web_input):\n",
    "#     draw = web_input['draw']\n",
    "    query = web_input['search[value]']\n",
    "    if len(query) == 0:\n",
    "        query = '*:*'\n",
    "    offset = web_input['start']\n",
    "    count = web_input['length']\n",
    "    return draw, query, offset, count\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# função construída no Assignment 1\n",
    "def search(query, offset, count, solr_endpoint):\n",
    "    \"\"\"\n",
    "    This function is responsible for hitting the solr endpoint and returning the results back.\n",
    "    \"\"\"\n",
    "    results = solr_endpoint.search(q=query, **{'start': int(offset), 'rows': int(count)})\n",
    "    print(\"Saw {0} result(s) for query {1}.\".format(len(results), query))\n",
    "    \n",
    "    formatted_hits = []\n",
    "    for hit in results.docs:\n",
    "        formatted_hits.append(\n",
    "            [hit['_news_title'], hit['_news_publisher'], CATEGORY[hit['_news_category'][0]], hit['_news_url']])\n",
    "    response = {'recordsFiltered': results.hits,\n",
    "                'data': formatted_hits}\n",
    "#     web.header('Content-Type', 'application/json')\n",
    "    return json.dumps(response)\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# new entity aware search function\n",
    "def search_entity_aware_index(query, offset, count, qf, time_in_ms):\n",
    "        \"\"\"\n",
    "        This function is responsible for hitting the solr endpoint and returning the results back.\n",
    "        \"\"\"\n",
    "        results = SOLR_ENTITYAWAREINDEX.search(q=query, **{'start': int(offset), 'rows': int(count),\n",
    "                                                           'segmentTerminatedEarly': 'true', 'timeAllowed': time_in_ms,\n",
    "                                                           'cache': 'false', 'qf': qf, 'pf': qf, 'debugQuery': 'true',\n",
    "                                                           'defType': 'edismax', 'ps': 10})\n",
    "        print(\"Saw {0} result(s) for query {1}.\".format(len(results), query))\n",
    "        print(results.debug)\n",
    "        \n",
    "        formatted_hits = []\n",
    "        for hit in results.docs:\n",
    "            formatted_hits.append(\n",
    "                [hit['_news_title'], hit['_news_publisher'], CATEGORY[hit['_news_category'][0]], hit['_news_url']])\n",
    "        response = {'recordsFiltered': results.hits,\n",
    "                    'data': formatted_hits}\n",
    "#         web.header('Content-Type', 'application/json')\n",
    "        return json.dumps(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleIndexSearchPage:\n",
    "    def GET(self):\n",
    "        return render.simpleIndexSearchPage()\n",
    "\n",
    "\n",
    "class EntityAwareSearch:\n",
    "    def GET(self):\n",
    "        return render.entityAwareSearchPage()\n",
    "\n",
    "\n",
    "class SearchSimpleIndex:\n",
    "    def GET(self):\n",
    "        draw, query, offset, count = get_web_input(web_input=web.input())\n",
    "\n",
    "        if query == '*:*':\n",
    "            return search_simple_index(query=query, offset=offset, count=count, draw=draw)\n",
    "\n",
    "        clauses = []\n",
    "        for token in word_tokenize(query):\n",
    "            clauses.append(\"+_text_:%s\" % token)\n",
    "        query = \" AND \".join(clauses)\n",
    "        return search_simple_index(query=query, offset=offset, count=count, draw=draw)\n",
    "\n",
    "\n",
    "class SearchEntityAwareIndex:\n",
    "    def accumulate_tags(self, list_of_tuples):\n",
    "        tokens, entities = zip(*list_of_tuples)\n",
    "        recognised = defaultdict(set)\n",
    "        duplicates = defaultdict(list)\n",
    "\n",
    "        for i, item in enumerate(entities):\n",
    "            duplicates[item].append(i)\n",
    "\n",
    "        for key, value in duplicates.items():\n",
    "            for k, g in groupby(enumerate(value), lambda x: x[0] - x[1]):\n",
    "                indices = list(map(itemgetter(1), g))\n",
    "                recognised[key].add(' '.join(tokens[index] for index in indices))\n",
    "        # recognised.pop('O', None)\n",
    "\n",
    "        recognised = dict(recognised)\n",
    "        ner_info = {}\n",
    "        for key, value in recognised.iteritems():\n",
    "            ner_info[key] = list(value)\n",
    "        return ner_info\n",
    "\n",
    "\n",
    "    def get_synonyms(self, text):\n",
    "        syn_set = []\n",
    "        for synset in wn.synsets(str):\n",
    "            for item in synset.lemma_names:\n",
    "                syn_set.append(item)\n",
    "        return syn_set\n",
    "\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        # title = unicode(query, \"utf-8\")\n",
    "        stop = stopwords.words('english') + list(string.punctuation)\n",
    "        return [i for i in word_tokenize(text) if i not in stop]\n",
    "\n",
    "\n",
    "    def build_clauses(self, prefix, tagged_segments):\n",
    "        clauses = []\n",
    "        for tagged_segment in tagged_segments:\n",
    "            tokens = self.tokenize_text(tagged_segment)\n",
    "            if len(tokens) == 1:\n",
    "                clauses.append(\"%s:%s\" % (prefix, tokens[0]))\n",
    "            else:\n",
    "                clauses.append(\"%s:\\\"%s\\\"\" % (prefix, \" \".join(tokens)))\n",
    "        return clauses\n",
    "\n",
    "\n",
    "    def GET(self):\n",
    "        draw, query, offset, count = get_web_input(web_input=web.input())\n",
    "\n",
    "        if query == '*:*':\n",
    "            return search_entity_aware_index(query=query, offset=offset, count=count,\n",
    "                          draw=draw, qf='_text_^1', time_in_ms=100)\n",
    "\n",
    "        # Utilize entity tagger to give out entities and remove unwanted tags\n",
    "        entity_tags = STANFORD_NER_SERVER.get_entities(query)\n",
    "        entity_tags = self.accumulate_tags(entity_tags)\n",
    "        print('Entity tags for query - %s, %s' % (query, entity_tags))\n",
    "\n",
    "        clauses = []\n",
    "        for entity_tag, tagged_segments in entity_tags.iteritems():\n",
    "            if entity_tag == 'PERSON':\n",
    "                clauses.extend(self.build_clauses(\"_news_title_person\", tagged_segments))\n",
    "            elif entity_tag == 'LOCATION':\n",
    "                clauses.extend(self.build_clauses(\"_news_title_location\", tagged_segments))\n",
    "            elif entity_tag == 'ORGANIZATION':\n",
    "                clauses.extend(self.build_clauses(\"_news_title_organization\", tagged_segments))\n",
    "                clauses.extend(self.build_clauses(\"_news_title_publisher\", tagged_segments))\n",
    "            else:\n",
    "                clauses.extend(self.build_clauses(\"_news_title\", tagged_segments))\n",
    "\n",
    "        query = \" AND \".join(clauses)\n",
    "        qf = '_news_title_person^10 _news_title_organization^5 _news_title_location^100 _news_title^2.0 _news_publisher^10.0'\n",
    "\n",
    "        return search_entity_aware_index(query=query, offset=offset, count=count, draw=draw, qf=qf, time_in_ms=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 2.6 minutes\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"\"\"Time to run: {} minutes\"\"\".format(round(total_time/60, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
