{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udemy: NLP - Natural Language Processing with Python (Notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/nlp-natural-language-processing-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_folder = os.path.abspath(os.getcwd()).replace(\"\\\\\", \"/\") + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) PYTHON TEXT BASICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1) Working with Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_0 = data_folder + '00-Python-Text-Basics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His name is Fred.\n",
      "His name is Fred.\n"
     ]
    }
   ],
   "source": [
    "name = 'Fred'\n",
    "\n",
    "# Using the old .format() method:\n",
    "print('His name is {}.'.format(name))\n",
    "\n",
    "# Using f-strings:\n",
    "print(f\"His name is {name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.txt\n",
    "Hello, this is a quick test file.\n",
    "This is the second line of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author     Topic    Pages  \n",
      "Twain      Rafting      601\n",
      "Feynman    Physics       95\n",
      "Hamilton   Mythology     144\n"
     ]
    }
   ],
   "source": [
    "library = [('Author', 'Topic', 'Pages'), ('Twain', 'Rafting', 601), ('Feynman', 'Physics', 95), ('Hamilton', 'Mythology', 144)]\n",
    "\n",
    "for book in library:\n",
    "    print(f'{book[0]:{10}} {book[1]:{8}} {book[2]:{7}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(('Twain', 'Rafting', 601)[1])\n",
    "len(str(('Twain', 'Rafting', 601)[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(str(i[2])) for i in library])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "max0 = max([len(i[0]) for i in library])\n",
    "max1 = max([len(i[1]) for i in library])\n",
    "max2 = max([len(str(i[2])) for i in library])\n",
    "\n",
    "for book in library:\n",
    "    print(f\"{book[0]:{max0} {book[1]:{max1} {book[2]:{max2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author\n",
      "Twain\n",
      "Feynman\n",
      "Hamilton\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Topic'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for book in library:\n",
    "    print(book[0])\n",
    "    \n",
    "('Author', 'Topic', 'Pages')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='test.txt' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile = open('test.txt')\n",
    "myfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, this is a quick test file.\\nThis is the second line of the file.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, this is a quick test file.\\nThis is the second line of the file.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('test.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2) Working with PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3) Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) NATURAL LANGUAGE PROCESSING BASICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_1 = data_folder + '01-NLP-Python-Basics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Spacy basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Na         PROPN      nsubj     \n",
      "minha      DET        det       \n",
      "atividade  NOUN       nmod:npmod\n",
      "actividade ADJ        flat:name \n",
      "profissional ADJ        amod      \n",
      "eu         PRON       flat:name \n",
      ",          PUNCT      punct     \n",
      "Tiago      PROPN      appos     \n",
      ",          PUNCT      punct     \n",
      "consegui   VERB       ROOT      \n",
      "aprofundar VERB       xcomp     \n",
      "melhor     ADV        advmod    \n",
      "os         DET        det       \n",
      "meus       DET        det       \n",
      "conhecimentos SYM        obj       \n",
      "em         ADP        case      \n",
      "\n",
      "          SPACE                \n",
      "Python     PROPN      nmod      \n",
      ",          PUNCT      punct     \n",
      "R          PROPN      ROOT      \n",
      "e          CCONJ      cc        \n",
      "gestão     NOUN       conj      \n",
      "de         ADP        case      \n",
      "equipas    NOUN       nmod      \n",
      "\n",
      "          SPACE                \n"
     ]
    }
   ],
   "source": [
    "# pt_core_news_sm\n",
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(\"\"\"Na minha atividade actividade profissional eu, Tiago, consegui aprofundar melhor os meus conhecimentos em \n",
    "Python, R e gestão de equipas\\n\"\"\")\n",
    "\n",
    "# Print each token separately\n",
    "for token in doc:\n",
    "#     print(token.text, token.pos_, token.dep_)\n",
    "    print(f'{token.text:{10}} {token.pos_:{10}} {token.dep_:{10}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "the DET det\n",
      "wonderful ADJ amod\n",
      "U.S. PROPN compound\n",
      "startup NOUN compound\n",
      "Ikari PROPN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM pobj\n"
     ]
    }
   ],
   "source": [
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(u'Tesla is looking at buying the wonderful U.S. startup Ikari for $6 million')\n",
    "\n",
    "# Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"dea26fa27d79492cacc2424980fb9c51-0\" class=\"displacy\" width=\"1250\" height=\"337.0\" direction=\"ltr\" style=\"max-width: none; height: 337.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">going</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">build</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">factory</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">6</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">million.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-0\" stroke-width=\"2px\" d=\"M70,202.0 C70,102.0 240.0,102.0 240.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,204.0 L62,192.0 78,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-1\" stroke-width=\"2px\" d=\"M170,202.0 C170,152.0 235.0,152.0 235.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M170,204.0 L162,192.0 178,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-2\" stroke-width=\"2px\" d=\"M370,202.0 C370,152.0 435.0,152.0 435.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M370,204.0 L362,192.0 378,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-3\" stroke-width=\"2px\" d=\"M270,202.0 C270,102.0 440.0,102.0 440.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M440.0,204.0 L448.0,192.0 432.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-4\" stroke-width=\"2px\" d=\"M570,202.0 C570,102.0 740.0,102.0 740.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570,204.0 L562,192.0 578,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-5\" stroke-width=\"2px\" d=\"M670,202.0 C670,152.0 735.0,152.0 735.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,204.0 L662,192.0 678,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-6\" stroke-width=\"2px\" d=\"M470,202.0 C470,52.0 745.0,52.0 745.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,204.0 L753.0,192.0 737.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-7\" stroke-width=\"2px\" d=\"M470,202.0 C470,2.0 850.0,2.0 850.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M850.0,204.0 L858.0,192.0 842.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-8\" stroke-width=\"2px\" d=\"M970,202.0 C970,102.0 1140.0,102.0 1140.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M970,204.0 L962,192.0 978,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-9\" stroke-width=\"2px\" d=\"M1070,202.0 C1070,152.0 1135.0,152.0 1135.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1070,204.0 L1062,192.0 1078,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dea26fa27d79492cacc2424980fb9c51-0-10\" stroke-width=\"2px\" d=\"M870,202.0 C870,52.0 1145.0,52.0 1145.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dea26fa27d79492cacc2424980fb9c51-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1145.0,204.0 L1153.0,192.0 1137.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to build a \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " factory for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 100})\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3) Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the toolkit and the full Porter Stemmer library\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer  # the 2nd Porter stemmer - more sofisticated\n",
    "\n",
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4) Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   561228191312463089     -PRON-\n",
      "saw          VERB   11925638236994514241   see\n",
      "eighteen     NUM    9609336664675087640    eighteen\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n",
      "!            PUNCT  17494803046312582752   !\n",
      ".            PUNCT  12646065887601541794   .\n",
      "Yeah         INTJ   11852442279192850303   yeah\n",
      ".            PUNCT  12646065887601541794   .\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"I saw eighteen mice today!. Yeah.\")\n",
    "\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I saw eighteen mice today!., Yeah.]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc2.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5) Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6) Vocabulary and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART OF SPEECH TAGGING (Pos Tag) & NAMED ENTITY RECOGNITION (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_2 = data_folder + '02-Parts-of-Speech-Tagging'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{90: 2, 84: 3, 92: 3, 100: 1, 85: 1, 94: 1, 97: 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"The quick brown fox jumped over the lazy dog's back.\")\n",
    "\n",
    "# Count the frequencies of different coarse-grained POS tags:\n",
    "POS_counts = doc.count_by(spacy.attrs.POS)\n",
    "POS_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(84, 3), (92, 3), (90, 2), (100, 1), (85, 1), (94, 1), (97, 1)] \n",
      "\n",
      "84. ADJ  : 3\n",
      "92. NOUN : 3\n",
      "90. DET  : 2\n",
      "100. VERB : 1\n",
      "85. ADP  : 1\n",
      "94. PART : 1\n",
      "97. PUNCT: 1\n"
     ]
    }
   ],
   "source": [
    "sorted_values = sorted(POS_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_values, \"\\n\")\n",
    "\n",
    "for pos, occ in sorted_values:\n",
    "    print(f'{pos}. {doc.vocab[pos].text:{5}}: {occ}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 dollars 4 6 20 31 MONEY\n",
      "Microsoft 11 12 53 62 ORG\n",
      "250 hours 20 22 91 100 TIME\n",
      "More than a day 24 28 102 117 DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"\"\"Can I please borrow 500 dollars from you to buy some Microsoft stock? I'll give it back in 250 hours.\n",
    "More than a day\"\"\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import SentenceSegmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_3 = data_folder + '03-Text-Classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.6931471805599453\n",
      "1.0986122886681098\n",
      "1.3862943611198906\n",
      "1.6094379124341003\n",
      "1.791759469228055\n",
      "1.9459101490553132\n",
      "2.0794415416798357\n",
      "2.1972245773362196\n"
     ]
    }
   ],
   "source": [
    "# tf-idf...\n",
    "for i in range(1, 10):\n",
    "    print(np.log(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.214608098422191\n",
      "6.907755278982137\n"
     ]
    }
   ],
   "source": [
    "print(np.log(500))\n",
    "print(np.log(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', \"ain't\", 'good', 'for', 'ya']\n",
      "[that, ai, n't, good, for, ya]\n"
     ]
    }
   ],
   "source": [
    "phrase = \"that ain't good for ya\"\n",
    "\n",
    "print(phrase.split())\n",
    "print([t for t in nlp(phrase)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEMANTICS AND SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_4 = data_folder + '04-Semantics-and-Sentiment-Analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692 54.59815003  2.71828183  7.3890561\n",
      " 20.08553692]\n",
      "114.98389973429897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.02364054, 0.06426166, 0.1746813 , 0.474833  , 0.02364054,\n",
       "       0.06426166, 0.1746813 ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax\n",
    "import numpy as np\n",
    "a = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n",
    "print(np.exp(a))\n",
    "print(np.sum(np.exp(a)) )\n",
    "\n",
    "np.exp(a) / np.sum(np.exp(a)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now we've been using spaCy's smallest English language model, ***en_core_web_sm (35MB)***, which provides vocabulary, syntax, and entities, but not vectors. To take advantage of built-in word vectors we'll need a larger library. We have a few options:\n",
    "\n",
    "- ***en_core_web_md (116MB) Vectors***: 685k keys, 20k unique vectors (300 dimensions)\n",
    "or\n",
    "- ***en_core_web_lg (812MB) Vectors***: 685k keys, 685k unique vectors (300 dimensions)\n",
    "\n",
    "<br>\n",
    "If you plan to rely heavily on word vectors, consider using spaCy's largest vector library containing over one million unique vectors:\n",
    "\n",
    "- **en_vectors_web_lg (631MB) Vectors**: 1.1m keys, 1.1m unique vectors (300 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[-0.038548  0.54252  -0.21843  -0.18855   0.073     0.1318   -0.10402\n",
      "  0.17231  -0.051587  2.8646  ]\n",
      "\n",
      "\n",
      "(300,)\n",
      "[-0.34868  -0.07772   0.17775  -0.094953 -0.45289   0.23779   0.20944\n",
      "  0.037886  0.035064  0.89901 ]\n"
     ]
    }
   ],
   "source": [
    "print(nlp('what').vector.shape)\n",
    "print(nlp('what').vector[:10])\n",
    "print(\"\\n\")\n",
    "print(nlp('fox').vector.shape)\n",
    "print(nlp('fox').vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[-0.19361399  0.2324     -0.02034    -0.1417515  -0.18994501  0.18479499\n",
      "  0.05271     0.10509799 -0.0082615   1.881805  ]\n"
     ]
    }
   ],
   "source": [
    "print(nlp('what fox').vector.shape)\n",
    "print(nlp('what fox').vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.19361399  0.2324     -0.02034    -0.1417515  -0.18994501  0.18479499\n",
      "  0.05271     0.10509799 -0.0082615   1.881805  ]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([nlp('what').vector[:10], nlp('fox').vector[:10]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# é a média entre os tokens\n",
    "np.mean([nlp('what').vector[:10], nlp('fox').vector[:10]], axis=0) == nlp('what fox').vector[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7359829457249657 \n",
      "\n",
      "0.9622129730267112\n",
      "tirado do DataCamp .- curso com a criadora do spacy \n",
      "There's no objective definition of similarity. Depends on the context and on what the application needs to do.\n",
      "\n",
      "Once you're getting serious about developing NLP applications that leverage semantic similarity, you might want\n",
      "to train vectors on your own data, or tweak the similarity algorithm.\n"
     ]
    }
   ],
   "source": [
    "print(nlp('lion').similarity(nlp('tiger')), \"\\n\")\n",
    "\n",
    "\n",
    "# like - hate - depends on context. change\n",
    "print(nlp('I like pizza.').similarity(nlp('I hate pizza.')))\n",
    "print(\"\"\"tirado do DataCamp .- curso com a criadora do spacy \n",
    "There's no objective definition of similarity. Depends on the context and on what the application needs to do.\n",
    "\n",
    "Once you're getting serious about developing NLP applications that leverage semantic similarity, you might want\n",
    "to train vectors on your own data, or tweak the similarity algorithm.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.512089421494053\n",
      "6.512089421494053\n"
     ]
    }
   ],
   "source": [
    "# aggregate the 300 dimension vectors into a Euclidian (L2) norm - square root of the sum-of-squared-vectors\n",
    "print(nlp('lion').vector_norm)\n",
    "print(sum(nlp('lion').vector**2)**0.5)\n",
    "# np.linalg.norm(nlp('lion').vector)  # igual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78808445]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# king - man + woman = queen\n",
    "kmw = nlp.vocab['king'].vector - nlp.vocab['man'].vector + nlp.vocab['woman'].vector\n",
    "queen = nlp.vocab['queen'].vector\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity([kmw], [queen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'queen', 'commoner', 'highness', 'prince', 'sultan', 'maharajas', 'princes', 'kumbia', 'kings']\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "\n",
    "# Now we find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "new_vector = king - man + woman\n",
    "computed_similarities = []\n",
    "\n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors and mixed-case words:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "                similarity = cosine_similarity([new_vector], [word.vector])\n",
    "                computed_similarities.append((word, similarity))\n",
    "\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passar o vetor para uma palavra (inventada?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['distutils', 'python', 'tkinter', 'nltk', 'numpy', 'scipy', 'capybara', 'reticulated', 'pygtk', 'argparse']\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "new_vector = nlp.vocab['Python'].vector\n",
    "\n",
    "# Now we find the closest vector in the vocabulary \n",
    "computed_similarities = []\n",
    "\n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors and mixed-case words:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "                similarity = cosine_similarity([new_vector], [word.vector])\n",
    "                computed_similarities.append((word, similarity))\n",
    "\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER - compound?\n",
    "a = {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOPIC MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_5 = data_folder + '05-Topic-Modeling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEEP LEARNING FOR NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_6 = data_folder + '06-Deep-Learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_cat = to_categorical(y)\n",
    "y_cat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_cat_train, y_cat_test = train_test_split(X, y_cat, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually when using Neural Networks, you will get better performance when you standardize the data. \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_object = MinMaxScaler()\n",
    "\n",
    "scaler_object.fit(X_train)  # fit apenas ao X_train por motivos de information leak\n",
    "scaled_X_train = scaler_object.transform(X_train)\n",
    "scaled_X_test = scaler_object.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41176471, 0.40909091, 0.55357143, 0.5       ],\n",
       "       [0.97058824, 0.45454545, 0.98214286, 0.83333333],\n",
       "       [0.38235294, 0.45454545, 0.60714286, 0.58333333],\n",
       "       [0.23529412, 0.68181818, 0.05357143, 0.04166667],\n",
       "       [1.        , 0.36363636, 1.        , 0.79166667]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52941176, 0.36363636, 0.64285714, 0.45833333],\n",
       "       [0.41176471, 0.81818182, 0.10714286, 0.08333333],\n",
       "       [1.        , 0.27272727, 1.03571429, 0.91666667],\n",
       "       [0.5       , 0.40909091, 0.60714286, 0.58333333],\n",
       "       [0.73529412, 0.36363636, 0.66071429, 0.54166667]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0909090909090908\n"
     ]
    }
   ],
   "source": [
    "print(scaled_X_train.max())\n",
    "print(scaled_X_test.max()) # mas tem que ser por motivos de information leak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the Network with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 139\n",
      "Trainable params: 139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit (Train) the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 0s - loss: 1.1063 - accuracy: 0.2100\n",
      "Epoch 2/200\n",
      " - 0s - loss: 1.1029 - accuracy: 0.3700\n",
      "Epoch 3/200\n",
      " - 0s - loss: 1.0996 - accuracy: 0.3800\n",
      "Epoch 4/200\n",
      " - 0s - loss: 1.0967 - accuracy: 0.4500\n",
      "Epoch 5/200\n",
      " - 0s - loss: 1.0942 - accuracy: 0.5100\n",
      "Epoch 6/200\n",
      " - 0s - loss: 1.0920 - accuracy: 0.5500\n",
      "Epoch 7/200\n",
      " - 0s - loss: 1.0901 - accuracy: 0.5600\n",
      "Epoch 8/200\n",
      " - 0s - loss: 1.0883 - accuracy: 0.5800\n",
      "Epoch 9/200\n",
      " - 0s - loss: 1.0865 - accuracy: 0.5800\n",
      "Epoch 10/200\n",
      " - 0s - loss: 1.0848 - accuracy: 0.5800\n",
      "Epoch 11/200\n",
      " - 0s - loss: 1.0831 - accuracy: 0.5800\n",
      "Epoch 12/200\n",
      " - 0s - loss: 1.0813 - accuracy: 0.5700\n",
      "Epoch 13/200\n",
      " - 0s - loss: 1.0795 - accuracy: 0.5900\n",
      "Epoch 14/200\n",
      " - 0s - loss: 1.0776 - accuracy: 0.6000\n",
      "Epoch 15/200\n",
      " - 0s - loss: 1.0758 - accuracy: 0.6800\n",
      "Epoch 16/200\n",
      " - 0s - loss: 1.0737 - accuracy: 0.6000\n",
      "Epoch 17/200\n",
      " - 0s - loss: 1.0716 - accuracy: 0.6400\n",
      "Epoch 18/200\n",
      " - 0s - loss: 1.0693 - accuracy: 0.6700\n",
      "Epoch 19/200\n",
      " - 0s - loss: 1.0671 - accuracy: 0.6100\n",
      "Epoch 20/200\n",
      " - 0s - loss: 1.0646 - accuracy: 0.6300\n",
      "Epoch 21/200\n",
      " - 0s - loss: 1.0622 - accuracy: 0.6400\n",
      "Epoch 22/200\n",
      " - 0s - loss: 1.0595 - accuracy: 0.6400\n",
      "Epoch 23/200\n",
      " - 0s - loss: 1.0568 - accuracy: 0.6400\n",
      "Epoch 24/200\n",
      " - 0s - loss: 1.0540 - accuracy: 0.6400\n",
      "Epoch 25/200\n",
      " - 0s - loss: 1.0512 - accuracy: 0.6400\n",
      "Epoch 26/200\n",
      " - 0s - loss: 1.0482 - accuracy: 0.6400\n",
      "Epoch 27/200\n",
      " - 0s - loss: 1.0448 - accuracy: 0.6400\n",
      "Epoch 28/200\n",
      " - 0s - loss: 1.0415 - accuracy: 0.6500\n",
      "Epoch 29/200\n",
      " - 0s - loss: 1.0383 - accuracy: 0.6500\n",
      "Epoch 30/200\n",
      " - 0s - loss: 1.0350 - accuracy: 0.6500\n",
      "Epoch 31/200\n",
      " - 0s - loss: 1.0308 - accuracy: 0.6500\n",
      "Epoch 32/200\n",
      " - 0s - loss: 1.0270 - accuracy: 0.6500\n",
      "Epoch 33/200\n",
      " - 0s - loss: 1.0230 - accuracy: 0.6500\n",
      "Epoch 34/200\n",
      " - 0s - loss: 1.0189 - accuracy: 0.6500\n",
      "Epoch 35/200\n",
      " - 0s - loss: 1.0146 - accuracy: 0.7700\n",
      "Epoch 36/200\n",
      " - 0s - loss: 1.0101 - accuracy: 0.7900\n",
      "Epoch 37/200\n",
      " - 0s - loss: 1.0052 - accuracy: 0.7600\n",
      "Epoch 38/200\n",
      " - 0s - loss: 1.0003 - accuracy: 0.7700\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.9952 - accuracy: 0.7900\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.9898 - accuracy: 0.7900\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.9845 - accuracy: 0.7900\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.9792 - accuracy: 0.7900\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.9737 - accuracy: 0.8100\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.9677 - accuracy: 0.7700\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.9620 - accuracy: 0.6500\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.9556 - accuracy: 0.7300\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.9489 - accuracy: 0.8300\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.9421 - accuracy: 0.8200\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.9353 - accuracy: 0.8200\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.9280 - accuracy: 0.8100\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.9209 - accuracy: 0.8000\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.9133 - accuracy: 0.7800\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.9065 - accuracy: 0.7900\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.8988 - accuracy: 0.8000\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.8913 - accuracy: 0.8000\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.8839 - accuracy: 0.8000\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.8767 - accuracy: 0.8000\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.8687 - accuracy: 0.8000\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.8608 - accuracy: 0.8000\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.8533 - accuracy: 0.8000\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.8454 - accuracy: 0.7900\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.8379 - accuracy: 0.7900\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.8304 - accuracy: 0.7900\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.8228 - accuracy: 0.8000\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.8152 - accuracy: 0.8000\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.8076 - accuracy: 0.8000\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.8004 - accuracy: 0.7900\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.7931 - accuracy: 0.7700\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.7865 - accuracy: 0.7700\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.7793 - accuracy: 0.7800\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.7724 - accuracy: 0.7800\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.7654 - accuracy: 0.7800\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.7587 - accuracy: 0.8000\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.7520 - accuracy: 0.8000\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.7455 - accuracy: 0.8000\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.7393 - accuracy: 0.8000\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.7331 - accuracy: 0.8000\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.7271 - accuracy: 0.8000\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.7213 - accuracy: 0.8000\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.7157 - accuracy: 0.8200\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.7102 - accuracy: 0.8200\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.7045 - accuracy: 0.8200\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.6991 - accuracy: 0.8200\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.6939 - accuracy: 0.8200\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.6888 - accuracy: 0.8300\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.6839 - accuracy: 0.8300\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.6791 - accuracy: 0.8300\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.6744 - accuracy: 0.8300\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.6700 - accuracy: 0.8300\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.6657 - accuracy: 0.8300\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.6617 - accuracy: 0.8400\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.6576 - accuracy: 0.8400\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.6537 - accuracy: 0.8600\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.6500 - accuracy: 0.8500\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.6464 - accuracy: 0.8400\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.6428 - accuracy: 0.8600\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.6393 - accuracy: 0.8400\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.6360 - accuracy: 0.8400\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.6327 - accuracy: 0.8300\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.6295 - accuracy: 0.8300\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.6265 - accuracy: 0.8300\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.6235 - accuracy: 0.8300\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.6205 - accuracy: 0.8300\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.6175 - accuracy: 0.8300\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.6147 - accuracy: 0.8300\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.6119 - accuracy: 0.8300\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.6092 - accuracy: 0.8300\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.6065 - accuracy: 0.8300\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.6038 - accuracy: 0.8300\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.6013 - accuracy: 0.8300\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.5986 - accuracy: 0.8400\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.5960 - accuracy: 0.8400\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.5934 - accuracy: 0.8400\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.5911 - accuracy: 0.8800\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.5886 - accuracy: 0.9000\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.5863 - accuracy: 0.9000\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.5844 - accuracy: 0.9000\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.5820 - accuracy: 0.9200\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.5799 - accuracy: 0.9200\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.5776 - accuracy: 0.9100\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.5751 - accuracy: 0.9100\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.5723 - accuracy: 0.6600\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.5692 - accuracy: 0.6600\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.5667 - accuracy: 0.6600\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.5639 - accuracy: 0.6600\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.5609 - accuracy: 0.6600\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.5576 - accuracy: 0.6600\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.5543 - accuracy: 0.6600\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.5510 - accuracy: 0.6600\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.5477 - accuracy: 0.8400\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.5445 - accuracy: 0.9100\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.5414 - accuracy: 0.9100\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.5378 - accuracy: 0.9100\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.5343 - accuracy: 0.9200\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.5304 - accuracy: 0.9200\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.5265 - accuracy: 0.9100\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.5228 - accuracy: 0.9100\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.5190 - accuracy: 0.9100\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.5152 - accuracy: 0.9000\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.5111 - accuracy: 0.9000\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.5069 - accuracy: 0.9000\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.5036 - accuracy: 0.9100\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.5002 - accuracy: 0.9200\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.4969 - accuracy: 0.9200\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.4936 - accuracy: 0.9200\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.4897 - accuracy: 0.9200\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.4863 - accuracy: 0.9000\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.4833 - accuracy: 0.8900\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.4803 - accuracy: 0.8900\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.4772 - accuracy: 0.8900\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.4740 - accuracy: 0.8900\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.4716 - accuracy: 0.8800\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.4679 - accuracy: 0.8900\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.4647 - accuracy: 0.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      " - 0s - loss: 0.4614 - accuracy: 0.8900\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.4589 - accuracy: 0.8900\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.4568 - accuracy: 0.8700\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.4541 - accuracy: 0.8600\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.4525 - accuracy: 0.8500\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.4500 - accuracy: 0.8500\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.4477 - accuracy: 0.8500\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.4453 - accuracy: 0.8500\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.4420 - accuracy: 0.8500\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.4386 - accuracy: 0.8500\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.4351 - accuracy: 0.8900\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.4316 - accuracy: 0.8900\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.4288 - accuracy: 0.8900\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.4258 - accuracy: 0.8900\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.4230 - accuracy: 0.8900\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.4206 - accuracy: 0.8900\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.4174 - accuracy: 0.9100\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.4151 - accuracy: 0.9100\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.4124 - accuracy: 0.8900\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.4097 - accuracy: 0.8900\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.4075 - accuracy: 0.8900\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.4046 - accuracy: 0.8900\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.4017 - accuracy: 0.8900\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.3989 - accuracy: 0.8900\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.3962 - accuracy: 0.8900\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.3935 - accuracy: 0.8900\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.3904 - accuracy: 0.9000\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.3876 - accuracy: 0.9300\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.3851 - accuracy: 0.9300\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.3830 - accuracy: 0.9300\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.3804 - accuracy: 0.9300\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.3774 - accuracy: 0.9300\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.3745 - accuracy: 0.9300\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.3722 - accuracy: 0.9300\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.3699 - accuracy: 0.9300\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.3676 - accuracy: 0.9300\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.3653 - accuracy: 0.9300\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.3629 - accuracy: 0.9300\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.3607 - accuracy: 0.9300\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.3586 - accuracy: 0.9300\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.3557 - accuracy: 0.9300\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.3537 - accuracy: 0.9300\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.3519 - accuracy: 0.9300\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.3490 - accuracy: 0.9300\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.3467 - accuracy: 0.9300\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.3443 - accuracy: 0.9300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ad49a96a48>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play around with number of epochs as well\n",
    "model.fit(scaled_X_train, y_cat_train, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting New Unseen Data**\n",
    "\n",
    "Let's see how we did by predicting on new data. Remember, our model has never seen the test data that we scaled previously! This process is the exact same process you would use on totally brand new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52941176, 0.36363636, 0.64285714, 0.45833333],\n",
       "       [0.41176471, 0.81818182, 0.10714286, 0.08333333],\n",
       "       [1.        , 0.27272727, 1.03571429, 0.91666667],\n",
       "       [0.5       , 0.40909091, 0.60714286, 0.58333333],\n",
       "       [0.73529412, 0.36363636, 0.66071429, 0.54166667]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.06768227e-02, 7.46325672e-01, 2.02997476e-01],\n",
       "       [9.92947698e-01, 6.91933557e-03, 1.32957008e-04],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01],\n",
       "       [4.66439389e-02, 6.48943543e-01, 3.04412544e-01],\n",
       "       [4.70929332e-02, 6.58475935e-01, 2.94431120e-01],\n",
       "       [9.81330454e-01, 1.80526637e-02, 6.16847188e-04],\n",
       "       [5.68658002e-02, 7.61135638e-01, 1.81998596e-01],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01],\n",
       "       [3.48592885e-02, 3.77847761e-01, 5.87292969e-01],\n",
       "       [5.06429709e-02, 7.45329261e-01, 2.04027697e-01],\n",
       "       [3.73332947e-02, 4.59844649e-01, 5.02822101e-01],\n",
       "       [9.77085352e-01, 2.22442020e-02, 6.70461624e-04],\n",
       "       [9.91768479e-01, 8.05437006e-03, 1.77095280e-04],\n",
       "       [9.80609000e-01, 1.88728124e-02, 5.18248824e-04],\n",
       "       [9.93060827e-01, 6.82041794e-03, 1.18782533e-04],\n",
       "       [4.99991253e-02, 7.27292776e-01, 2.22708121e-01],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01],\n",
       "       [5.00011072e-02, 7.27346361e-01, 2.22652569e-01],\n",
       "       [4.92698736e-02, 7.08479047e-01, 2.42251083e-01],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01],\n",
       "       [9.80606616e-01, 1.87936276e-02, 5.99749270e-04],\n",
       "       [3.88714150e-02, 5.00632226e-01, 4.60496306e-01],\n",
       "       [9.79975283e-01, 1.93462204e-02, 6.78480486e-04],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01],\n",
       "       [4.64746393e-02, 6.45402074e-01, 3.08123320e-01],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01],\n",
       "       [9.64055359e-01, 3.44830155e-02, 1.46164151e-03],\n",
       "       [9.74135280e-01, 2.50383057e-02, 8.26479634e-04],\n",
       "       [9.93353724e-01, 6.53768750e-03, 1.08566106e-04],\n",
       "       [9.97413576e-01, 2.56192405e-03, 2.45333722e-05],\n",
       "       [5.12808375e-02, 7.65217304e-01, 1.83501929e-01],\n",
       "       [9.87154484e-01, 1.25262802e-02, 3.19279701e-04],\n",
       "       [9.82978880e-01, 1.65266283e-02, 4.94553533e-04],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01],\n",
       "       [5.07304706e-02, 7.47916758e-01, 2.01352835e-01],\n",
       "       [9.90469813e-01, 9.32030380e-03, 2.09814854e-04],\n",
       "       [9.92291629e-01, 7.56524038e-03, 1.43099023e-04],\n",
       "       [9.97585416e-01, 2.39375490e-03, 2.08309866e-05],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01],\n",
       "       [5.92954904e-02, 7.47736812e-01, 1.92967758e-01],\n",
       "       [4.94749136e-02, 7.13628769e-01, 2.36896217e-01],\n",
       "       [9.93717670e-01, 6.17537182e-03, 1.06958105e-04],\n",
       "       [9.93663192e-01, 6.22759713e-03, 1.09206143e-04],\n",
       "       [5.06780595e-02, 7.46362388e-01, 2.02959538e-01],\n",
       "       [4.34000343e-02, 5.85003853e-01, 3.71596128e-01],\n",
       "       [3.93246301e-02, 5.12722373e-01, 4.47953016e-01],\n",
       "       [5.03620505e-02, 7.37259448e-01, 2.12378502e-01],\n",
       "       [3.48246694e-02, 3.63085717e-01, 6.02089584e-01]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spits out probabilities by default.\n",
    "model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       0, 2, 1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints classes\n",
    "model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.predict(scaled_X_test)) == len(model.predict_classes(scaled_X_test)) == 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating Model Performance**\n",
    "\n",
    "So how well did we do? How do we actually measure \"well\". Is 95% accuracy good enough? It all depends on the situation. Also we need to take into account things like recall and precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 780us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3144051456451416, 0.8999999761581421]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=scaled_X_test, y=y_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       0, 2, 1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(scaled_X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(y_cat_test[:5])\n",
    "y_cat_test.argmax(axis=1)  # para comparar com o mesmo formato das predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  0,  0],\n",
       "       [ 0, 14,  1],\n",
       "       [ 0,  4, 12]], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix(y_cat_test.argmax(axis=1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.78      0.93      0.85        15\n",
      "           2       0.92      0.75      0.83        16\n",
      "\n",
      "    accuracy                           0.90        50\n",
      "   macro avg       0.90      0.89      0.89        50\n",
      "weighted avg       0.91      0.90      0.90        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_cat_test.argmax(axis=1),predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving and Loading Models**\n",
    "\n",
    "Now that we have a model trained, let's see how we can save and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       0, 2, 1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "newmodel = load_model('myfirstmodel.h5')\n",
    "\n",
    "newmodel.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):    \n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()    \n",
    "    return str_text\n",
    "\n",
    "##############################################################\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger','ner'])\n",
    "nlp.max_length = 1198623\n",
    "\n",
    "##############################################################\n",
    "\n",
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in \n",
    "            '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']\n",
    "\n",
    "d = read_file(data_folder_6 + '/melville-moby_dick.txt')\n",
    "tokens = separate_punc(d)\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# organize into sequences of tokens\n",
    "train_len = 25+1 # 50 training words , then one target word\n",
    "\n",
    "# Empty list of sequences\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):    \n",
    "    # Grab train_len# amount of characters\n",
    "    seq = tokens[i-train_len:i]    \n",
    "    # Add to list of sequences\n",
    "    text_sequences.append(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# moby dick trained model (>3hours) - usar o já existente\n",
    "\n",
    "from keras.models import load_model\n",
    "from pickle import load\n",
    "\n",
    "model = load_model(data_folder_6 + '/epochBIG.h5')\n",
    "\n",
    "tokenizer = load(open(data_folder_6 + '/epochBIG', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    model : model that was trained on text data\n",
    "    tokenizer : tokenizer that was fit on text data\n",
    "    seq_len : length of training sequence\n",
    "    seed_text : raw string text to serve as the seed\n",
    "    num_gen_words : number of words to be generated by model\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # Pad sequences to our trained rate (50 words in the video)\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        # Predict Class Probabilities for each word\n",
    "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n",
    "        \n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Make it look like a sentence.\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stranger that stubb vowed he recognised his cutting spade pole entangled in the lines that were knotted round the tail of one of these whales there'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0, len(text_sequences))\n",
    "random_seed_text = text_sequences[random_pick]\n",
    "seed_text = ' '.join(random_seed_text)\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i thought i did boys tail men wondrous madness he does very laid oh ye fly me much with a greatest brother of jackasses out the latter 's castaways let very fast when well i 'd go a canakin of our walls and what and personally get to go a\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definido previamente (ver outro ipynb)\n",
    "seq_len = 25\n",
    "\n",
    "generate_text(model, tokenizer, seq_len, seed_text=seed_text, num_gen_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
